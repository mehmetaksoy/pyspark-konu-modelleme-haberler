# PySpark ile Haber BaÅŸlÄ±klarÄ± Ãœzerinde Konu Modelleme (K-means Clustering)

Bu proje, "abcnews-date-text.csv" veri setindeki haber baÅŸlÄ±klarÄ±nÄ± kullanarak DoÄŸal Dil Ä°ÅŸleme (NLP) teknikleri ve PySpark Ã¼zerinde K-means kÃ¼meleme algoritmasÄ± ile konu modellemesi yapmayÄ± amaÃ§lamaktadÄ±r. Proje, metin verilerinin temizlenmesi, Ã¶zellik Ã§Ä±karÄ±mÄ± (TF-IDF ve Word2Vec) ve kÃ¼meleme adÄ±mlarÄ±nÄ± iÃ§ermektedir. Bu notebook, bir Databricks ortamÄ±nda Ã§alÄ±ÅŸtÄ±rÄ±lmak Ã¼zere hazÄ±rlanmÄ±ÅŸtÄ±r.

## ğŸ¯ Projenin AmacÄ±

Temel amaÃ§, geniÅŸ bir haber baÅŸlÄ±ÄŸÄ± koleksiyonunu anlamsal olarak benzer gruplara (konulara) ayÄ±rmaktÄ±r. Bu sayede, veri setindeki ana temalar ve konular hakkÄ±nda fikir edinilebilir.

## ğŸ› ï¸ KullanÄ±lan Teknolojiler ve YÃ¶ntemler

* **Dil:** Python
* **Ana KÃ¼tÃ¼phaneler:**
    * `PySpark`: BÃ¼yÃ¼k veri iÅŸleme ve daÄŸÄ±tÄ±k hesaplama iÃ§in.
        * `pyspark.sql`: DataFrame iÅŸlemleri iÃ§in.
        * `pyspark.ml.feature`: Metin Ã¶zellik Ã§Ä±karma (Tokenizer, StopWordsRemover, CountVectorizer, IDF, Word2Vec) iÃ§in.
        * `pyspark.ml.clustering`: K-means kÃ¼meleme iÃ§in.
        * `pyspark.ml.evaluation`: KÃ¼meleme deÄŸerlendirme (ClusteringEvaluator) iÃ§in.
    * `nltk`: Kelime kÃ¶klerini bulma (SnowballStemmer) iÃ§in.
    * `pandas`, `numpy`: Veri manipÃ¼lasyonu ve sayÄ±sal iÅŸlemler iÃ§in (Ã¶zellikle elbow metodu sonuÃ§larÄ±nÄ± gÃ¶stermek gibi yardÄ±mcÄ± gÃ¶revlerde).
    * `matplotlib`, `seaborn`: (Notebook'ta import edilmiÅŸ ancak aktif grafik Ã§izimi gÃ¶rÃ¼lmemiÅŸtir, potansiyel gÃ¶rselleÅŸtirmeler iÃ§in dahil edilebilir).
* **Veri KaynaÄŸÄ±:** `abcnews-date-text.csv` (ABC News haber baÅŸlÄ±klarÄ±).
* **NLP Ã–n Ä°ÅŸleme AdÄ±mlarÄ±:**
    1.  Metin Temizleme: KÃ¼Ã§Ã¼k harfe Ã§evirme, boÅŸluklarÄ± ve noktalama iÅŸaretlerini kaldÄ±rma.
    2.  Tokenizasyon: Metinleri kelimelere ayÄ±rma.
    3.  Stopwords (Etkisiz Kelimeler) KaldÄ±rma.
    4.  Stemming (KÃ¶k Bulma): Ä°ngilizce kelimeler iÃ§in SnowballStemmer.
* **Ã–zellik Ã‡Ä±karma (VektÃ¶rleÅŸtirme):**
    1.  **TF-IDF (Term Frequency-Inverse Document Frequency)**
    2.  **Word2Vec** (Alternatif bir yÃ¶ntem olarak)
* **KÃ¼meleme AlgoritmasÄ±:** K-means
* **DeÄŸerlendirme MetriÄŸi:** Silhouette Score
* **Optimal KÃ¼me SayÄ±sÄ± Belirleme:** Elbow Method (Word2Vec iÃ§in uygulanmÄ±ÅŸ)
* **Ã‡alÄ±ÅŸma OrtamÄ±:** Databricks ( `%sh`, `%fs` ve `dbutils` komutlarÄ± kullanÄ±lmÄ±ÅŸtÄ±r).

## ğŸ“ Veri Seti

Projede kullanÄ±lan veri seti, Avustralya ABC News tarafÄ±ndan yayÄ±nlanan haber baÅŸlÄ±klarÄ±nÄ± iÃ§ermektedir. Veri seti `publish_date` (yayÄ±n tarihi) ve `headline_text` (haber baÅŸlÄ±ÄŸÄ± metni) olmak Ã¼zere iki sÃ¼tundan oluÅŸur. Veri seti, bir Google Drive linkinden indirilmektedir.

## ğŸŒŠ Ä°ÅŸ AkÄ±ÅŸÄ± (Pipeline)

1.  **Veri YÃ¼kleme ve HazÄ±rlÄ±k:**
    * Haber baÅŸlÄ±klarÄ± veri seti `%sh curl` komutu ile Google Drive'dan Databricks sÃ¼rÃ¼cÃ¼ dÃ¼ÄŸÃ¼mÃ¼nÃ¼n yerel `/tmp` klasÃ¶rÃ¼ne indirilir.
    * `dbutils.fs.mv` komutu ile dosya DBFS'e (`dbfs:/datasets/`) taÅŸÄ±nÄ±r.
    * Veri, `spark.read.load` ile bir Spark DataFrame'e yÃ¼klenir.
    * Veri setinin yapÄ±sÄ±, boyutu incelenir ve `headline_text` bazÄ±nda yinelenen kayÄ±tlar temizlenir. Eksik deÄŸer kontrolÃ¼ yapÄ±lÄ±r.

2.  **DoÄŸal Dil Ã–n Ä°ÅŸleme (`clean_text` fonksiyonu):**
    * `headline_text` sÃ¼tunundaki metinler kÃ¼Ã§Ã¼k harfe Ã§evrilir.
    * BaÅŸtaki/sondaki boÅŸluklar ve noktalama iÅŸaretleri kaldÄ±rÄ±lÄ±r.
    * Metinler, PySpark `Tokenizer` ile kelimelere (token) ayrÄ±lÄ±r.
    * Ä°ngilizce iÃ§in yaygÄ±n etkisiz kelimeler (stopwords) PySpark `StopWordsRemover` ile Ã§Ä±karÄ±lÄ±r.
    * NLTK `SnowballStemmer` kullanÄ±larak kelimeler kÃ¶klerine indirgenir (PySpark UDF ile uygulanÄ±r).

3.  **Ã–zellik Ã‡Ä±karma (Feature Engineering):**
    * **TF-IDF VektÃ¶rleri (`extract_tfidf_features` fonksiyonu):**
        * Ã–n iÅŸlenmiÅŸ (kÃ¶klerine ayrÄ±lmÄ±ÅŸ) metinlerden, PySpark `CountVectorizer` ve `IDF` kullanÄ±larak TF-IDF vektÃ¶rleri oluÅŸturulur.
        * OluÅŸan vektÃ¶rlerden sÄ±fÄ±r uzunluklu olanlar (yani hiÃ§bir Ã¶zellik iÃ§ermeyenler) filtrelenir.
    * **Word2Vec VektÃ¶rleri (`extract_w2v_features` fonksiyonu):**
        * (Etkisiz kelimeleri Ã§Ä±karÄ±lmÄ±ÅŸ ancak kÃ¶klerine ayrÄ±lmamÄ±ÅŸ) metinlerden PySpark `Word2Vec` modeli eÄŸitilir ve kelime/belge vektÃ¶rleri elde edilir.
        * EÄŸitilen model ile kelime sinonimleri bulma Ã¶rneÄŸi gÃ¶sterilir.

4.  **K-means KÃ¼meleme (`k_means` fonksiyonu):**
    * TF-IDF Ã¶zellikleri kullanÄ±larak PySpark `KMeans` ile model eÄŸitilir.
    * Belirlenen sayÄ±da (`N_CLUSTERS`) kÃ¼me oluÅŸturulur.

5.  **KÃ¼meleme DeÄŸerlendirmesi:**
    * `evaluate_k_means` fonksiyonu ile Silhouette Score kullanÄ±larak kÃ¼meleme kalitesi deÄŸerlendirilir.
    * Her kÃ¼medeki Ã¶ÄŸe sayÄ±sÄ± ve her kÃ¼meden Ã¶rnek haber baÅŸlÄ±klarÄ± incelenir.

6.  **Optimal KÃ¼me SayÄ±sÄ± (Elbow Method - Word2Vec iÃ§in):**
    * `elbow_method` fonksiyonu ile Word2Vec Ã¶zellikleri Ã¼zerinde farklÄ± K deÄŸerleri iÃ§in K-means Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r ve Sum of Squared Errors (SSE) deÄŸerleri toplanarak dirsek grafiÄŸi iÃ§in veri hazÄ±rlanÄ±r.

## ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma (Databricks OrtamÄ±nda)

1.  **Notebook Ä°Ã§e Aktarma:** Bu `nlp_kmeans.ipynb` dosyasÄ±nÄ± Databricks Ã§alÄ±ÅŸma alanÄ±nÄ±za (workspace) import edin.
2.  **KÃ¼tÃ¼phane Kurulumu:**
    * Notebook'un ilk hÃ¼cresindeki `pip install nltk` komutu, NLTK kÃ¼tÃ¼phanesini kÃ¼me (cluster) ortamÄ±na kuracaktÄ±r. EÄŸer kÃ¼me yapÄ±landÄ±rmasÄ±nda kÃ¼tÃ¼phane zaten kurulu deÄŸilse bu adÄ±m gereklidir.
    * DiÄŸer kÃ¼tÃ¼phaneler (`pyspark`, `pandas` vb.) genellikle Databricks ortamlarÄ±nda standart olarak bulunur.
3.  **KÃ¼me (Cluster) HazÄ±rlÄ±ÄŸÄ±:** Notebook'u Ã§alÄ±ÅŸtÄ±racaÄŸÄ±nÄ±z Databricks kÃ¼mesinin NLTK kÃ¼tÃ¼phanesini ve baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± iÃ§erdiÄŸinden emin olun.
4.  **Ã‡alÄ±ÅŸtÄ±rma:**
    * HÃ¼creleri sÄ±rayla Ã§alÄ±ÅŸtÄ±rÄ±n.
    * Veri indirme ve DBFS'e taÅŸÄ±ma iÅŸlemleri notebook iÃ§indeki `%sh` ve `dbutils` komutlarÄ± ile otomatik olarak yapÄ±lÄ±r.
    * NLP Ã¶n iÅŸleme, Ã¶zellik Ã§Ä±karma ve kÃ¼meleme adÄ±mlarÄ± Spark Ã¼zerinde daÄŸÄ±tÄ±k olarak Ã§alÄ±ÅŸacaktÄ±r.

## ğŸ“Š SonuÃ§larÄ±n YorumlanmasÄ±

* **Silhouette Score:** Elde edilen skorun (-1 ile 1 arasÄ±nda) 1'e yakÄ±n olmasÄ± daha iyi bir kÃ¼melemeyi, 0'a yakÄ±n olmasÄ± kÃ¼melerin Ã¼st Ã¼ste bindiÄŸini, negatif deÄŸerler ise yanlÄ±ÅŸ kÃ¼melemeyi iÅŸaret eder. TF-IDF ile elde edilen skor (~0.039) Ã§ok yÃ¼ksek olmasa da, metin kÃ¼melemede bu tÃ¼r skorlar gÃ¶rÃ¼lebilir ve iyileÅŸtirme potansiyeli olduÄŸunu gÃ¶sterir.
* **KÃ¼me Ä°Ã§erikleri:** Her kÃ¼meden rastgele seÃ§ilen haber baÅŸlÄ±klarÄ± incelenerek kÃ¼melerin anlamsal tutarlÄ±lÄ±ÄŸÄ± ve hangi konularÄ± temsil ettiÄŸi hakkÄ±nda fikir edinilebilir.
* **Elbow Method GrafiÄŸi:** Word2Vec iÃ§in Ã§izilecek dirsek grafiÄŸi (bu notebook'ta grafik Ã§izimi eklenmemiÅŸ ama verisi toplanmÄ±ÅŸ), SSE'deki azalmanÄ±n yavaÅŸladÄ±ÄŸÄ± "dirsek" noktasÄ±nÄ± bularak optimal K sayÄ±sÄ±nÄ± tahmin etmeye yardÄ±mcÄ± olur.

## ğŸ’¡ OlasÄ± GeliÅŸtirmeler

* FarklÄ± NLP Ã¶n iÅŸleme adÄ±mlarÄ± denenebilir (Ã¶rneÄŸin, Lemmatization yerine Stemming veya tam tersi, farklÄ± n-gram aralÄ±klarÄ±).
* TF-IDF iÃ§in `vocabSize`, `minDF` gibi parametreler optimize edilebilir.
* Word2Vec iÃ§in `vectorSize`, `minCount`, `windowSize` gibi parametreler optimize edilebilir.
* K-means dÄ±ÅŸÄ±ndaki kÃ¼meleme algoritmalarÄ± (Ã¶rneÄŸin, Latent Dirichlet Allocation - LDA, DBSCAN) denenebilir.
* KÃ¼melerin daha iyi yorumlanabilmesi iÃ§in her kÃ¼medeki en karakteristik kelimeler (Ã¶rneÄŸin, TF-IDF skorlarÄ± en yÃ¼ksek kelimeler) Ã§Ä±karÄ±labilir.
* SonuÃ§larÄ±n gÃ¶rselleÅŸtirilmesi (Ã¶rneÄŸin, t-SNE veya UMAP ile boyut indirgeme sonrasÄ± kÃ¼me grafikleri).